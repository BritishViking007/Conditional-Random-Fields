{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The address contains the base for out dataset\n",
    "base_url = \"http://www.thelatinlibrary.com/\"\n",
    "home_content = urlopen(base_url)\n",
    "\n",
    "#use BeautifulSoup to take out relevant data\n",
    "soup = BeautifulSoup(home_content, \"lxml\")\n",
    "author_page_links = soup.find_all(\"a\")\n",
    "author_pages = [ap[\"href\"] for i,ap in enumerate(author_page_links) if i < 49]\n",
    "ap_content = list()\n",
    "texts = list()\n",
    "\n",
    "# Take the content from first 49 pages\n",
    "for ap in author_pages:\n",
    "    ap_content.append(urlopen(base_url + ap))\n",
    "\n",
    "book_links = list()\n",
    "\n",
    "for path,content in zip(author_pages, ap_content):\n",
    "    author_name = path.split(\".\")[0]\n",
    "    ap_soup = BeautifulSoup(content, \"lxml\")\n",
    "    book_links += ([link for link in ap_soup.find_all(\"a\", {\"href\": True}) if author_name in link[\"href\"]])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"ammianus/14.shtml\">Liber XIV</a>\n"
     ]
    }
   ],
   "source": [
    "print(book_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting content 200 of 200\r"
     ]
    }
   ],
   "source": [
    "num_pages = 200\n",
    "\n",
    "for i,bl in enumerate(book_links[:num_pages]):\n",
    "    print(\"Getting content \" + str(i+1) + \" of \" + str(num_pages), end=\"\\r\", flush=True)\n",
    "    try:\n",
    "        content = urlopen(base_url + bl[\"href\"]).read()\n",
    "        texts.append(content)\n",
    "    except HTTPError as err:\n",
    "        print(\"Unable to retrieve\" + bl[\"href\"] + \".\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tentis igitur regis utriusque legatis et negotio tectius diu pensato cum pacem oportere tribui quae iustis condicionibus petebatur eamque ex re tum fore sententiarum via concinens adprobasset advocato in contionem exercitu imperator pro tempore pauca dicturus tribunali adsistens circumdatus potestatum coetu celsarum ad hunc disservit modum nemo quaeso miretur si post exsudatos labores itinerum longos congestosque adfatim commeatus fiducia vestri ductante barbaricos pagos adventans velut mutato repente consilio ad placidiora deverti\n"
     ]
    }
   ],
   "source": [
    "# Training Conditional Random Field for segmenting images\n",
    "\n",
    "\n",
    "sentences = list()\n",
    "\n",
    "for i,text in enumerate(texts):\n",
    "    print(\"Document \" + str(i+1) + \" of \" + str(len(texts)), end=\"\\r\", flush=True)\n",
    "    textSoup = BeautifulSoup(text,\"lxml\")\n",
    "    paragraphs = textSoup.find_all(\"p\",attrs={\"class\":None})\n",
    "    prepared = (\"\".join([p.text.strip().lower() for p in paragraphs[1:-1]]))\n",
    "    for t in prepared.split(\".\"):\n",
    "        part = \"\".join([c for c in t if c.isalpha() or c.isspace()])\n",
    "        sentences.append(part.strip())\n",
    "        \n",
    "sentences = [s for s in sentences if len(s) > 5]\n",
    "\n",
    "print(sentences[200])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t', 0), ('e', 0), ('n', 0), ('t', 0), ('i', 0), ('s', 0), ('i', 1), ('g', 0), ('i', 0), ('t', 0), ('u', 0), ('r', 0), ('r', 1), ('e', 0), ('g', 0), ('i', 0), ('s', 0), ('u', 1), ('t', 0), ('r', 0), ('i', 0), ('u', 0), ('s', 0), ('q', 0), ('u', 0), ('e', 0), ('l', 1), ('e', 0), ('g', 0), ('a', 0), ('t', 0), ('i', 0), ('s', 0), ('e', 1), ('t', 0), ('n', 1), ('e', 0), ('g', 0), ('o', 0), ('t', 0), ('i', 0), ('o', 0), ('t', 1), ('e', 0), ('c', 0), ('t', 0), ('i', 0), ('u', 0), ('s', 0), ('d', 1), ('i', 0), ('u', 0), ('p', 1), ('e', 0), ('n', 0), ('s', 0), ('a', 0), ('t', 0), ('o', 0), ('c', 1), ('u', 0), ('m', 0), ('p', 1), ('a', 0), ('c', 0), ('e', 0), ('m', 0), ('o', 1), ('p', 0), ('o', 0), ('r', 0), ('t', 0), ('e', 0), ('r', 0), ('e', 0), ('t', 1), ('r', 0), ('i', 0), ('b', 0), ('u', 0), ('i', 0), ('q', 1), ('u', 0), ('a', 0), ('e', 0), ('i', 1), ('u', 0), ('s', 0), ('t', 0), ('i', 0), ('s', 0), ('c', 1), ('o', 0), ('n', 0), ('d', 0), ('i', 0), ('c', 0), ('i', 0), ('o', 0), ('n', 0), ('i', 0), ('b', 0), ('u', 0), ('s', 0), ('p', 1), ('e', 0), ('t', 0), ('e', 0), ('b', 0), ('a', 0), ('t', 0), ('u', 0), ('r', 0), ('e', 1), ('a', 0), ('m', 0), ('q', 0), ('u', 0), ('e', 0), ('e', 1), ('x', 0), ('r', 1), ('e', 0), ('t', 1), ('u', 0), ('m', 0), ('f', 1), ('o', 0), ('r', 0), ('e', 0), ('s', 1), ('e', 0), ('n', 0), ('t', 0), ('e', 0), ('n', 0), ('t', 0), ('i', 0), ('a', 0), ('r', 0), ('u', 0), ('m', 0), ('v', 1), ('i', 0), ('a', 0), ('c', 1), ('o', 0), ('n', 0), ('c', 0), ('i', 0), ('n', 0), ('e', 0), ('n', 0), ('s', 0), ('a', 1), ('d', 0), ('p', 0), ('r', 0), ('o', 0), ('b', 0), ('a', 0), ('s', 0), ('s', 0), ('e', 0), ('t', 0), ('a', 1), ('d', 0), ('v', 0), ('o', 0), ('c', 0), ('a', 0), ('t', 0), ('o', 0), ('i', 1), ('n', 0), ('c', 1), ('o', 0), ('n', 0), ('t', 0), ('i', 0), ('o', 0), ('n', 0), ('e', 0), ('m', 0), ('e', 1), ('x', 0), ('e', 0), ('r', 0), ('c', 0), ('i', 0), ('t', 0), ('u', 0), ('i', 1), ('m', 0), ('p', 0), ('e', 0), ('r', 0), ('a', 0), ('t', 0), ('o', 0), ('r', 0), ('p', 1), ('r', 0), ('o', 0), ('t', 1), ('e', 0), ('m', 0), ('p', 0), ('o', 0), ('r', 0), ('e', 0), ('p', 1), ('a', 0), ('u', 0), ('c', 0), ('a', 0), ('d', 1), ('i', 0), ('c', 0), ('t', 0), ('u', 0), ('r', 0), ('u', 0), ('s', 0), ('t', 1), ('r', 0), ('i', 0), ('b', 0), ('u', 0), ('n', 0), ('a', 0), ('l', 0), ('i', 0), ('a', 1), ('d', 0), ('s', 0), ('i', 0), ('s', 0), ('t', 0), ('e', 0), ('n', 0), ('s', 0), ('c', 1), ('i', 0), ('r', 0), ('c', 0), ('u', 0), ('m', 0), ('d', 0), ('a', 0), ('t', 0), ('u', 0), ('s', 0), ('p', 1), ('o', 0), ('t', 0), ('e', 0), ('s', 0), ('t', 0), ('a', 0), ('t', 0), ('u', 0), ('m', 0), ('c', 1), ('o', 0), ('e', 0), ('t', 0), ('u', 0), ('c', 1), ('e', 0), ('l', 0), ('s', 0), ('a', 0), ('r', 0), ('u', 0), ('m', 0), ('a', 1), ('d', 0), ('h', 1), ('u', 0), ('n', 0), ('c', 0), ('d', 1), ('i', 0), ('s', 0), ('s', 0), ('e', 0), ('r', 0), ('v', 0), ('i', 0), ('t', 0), ('m', 1), ('o', 0), ('d', 0), ('u', 0), ('m', 0), ('n', 1), ('e', 0), ('m', 0), ('o', 0), ('q', 1), ('u', 0), ('a', 0), ('e', 0), ('s', 0), ('o', 0), ('m', 1), ('i', 0), ('r', 0), ('e', 0), ('t', 0), ('u', 0), ('r', 0), ('s', 1), ('i', 0), ('p', 1), ('o', 0), ('s', 0), ('t', 0), ('e', 1), ('x', 0), ('s', 0), ('u', 0), ('d', 0), ('a', 0), ('t', 0), ('o', 0), ('s', 0), ('l', 1), ('a', 0), ('b', 0), ('o', 0), ('r', 0), ('e', 0), ('s', 0), ('i', 1), ('t', 0), ('i', 0), ('n', 0), ('e', 0), ('r', 0), ('u', 0), ('m', 0), ('l', 1), ('o', 0), ('n', 0), ('g', 0), ('o', 0), ('s', 0), ('c', 1), ('o', 0), ('n', 0), ('g', 0), ('e', 0), ('s', 0), ('t', 0), ('o', 0), ('s', 0), ('q', 0), ('u', 0), ('e', 0), ('a', 1), ('d', 0), ('f', 0), ('a', 0), ('t', 0), ('i', 0), ('m', 0), ('c', 1), ('o', 0), ('m', 0), ('m', 0), ('e', 0), ('a', 0), ('t', 0), ('u', 0), ('s', 0), ('f', 1), ('i', 0), ('d', 0), ('u', 0), ('c', 0), ('i', 0), ('a', 0), ('v', 1), ('e', 0), ('s', 0), ('t', 0), ('r', 0), ('i', 0), ('d', 1), ('u', 0), ('c', 0), ('t', 0), ('a', 0), ('n', 0), ('t', 0), ('e', 0), ('b', 1), ('a', 0), ('r', 0), ('b', 0), ('a', 0), ('r', 0), ('i', 0), ('c', 0), ('o', 0), ('s', 0), ('p', 1), ('a', 0), ('g', 0), ('o', 0), ('s', 0), ('a', 1), ('d', 0), ('v', 0), ('e', 0), ('n', 0), ('t', 0), ('a', 0), ('n', 0), ('s', 0), ('v', 1), ('e', 0), ('l', 0), ('u', 0), ('t', 0), ('m', 1), ('u', 0), ('t', 0), ('a', 0), ('t', 0), ('o', 0), ('r', 1), ('e', 0), ('p', 0), ('e', 0), ('n', 0), ('t', 0), ('e', 0), ('c', 1), ('o', 0), ('n', 0), ('s', 0), ('i', 0), ('l', 0), ('i', 0), ('o', 0), ('a', 1), ('d', 0), ('p', 1), ('l', 0), ('a', 0), ('c', 0), ('i', 0), ('d', 0), ('i', 0), ('o', 0), ('r', 0), ('a', 0), ('d', 1), ('e', 0), ('v', 0), ('e', 0), ('r', 0), ('t', 0), ('i', 0)]\n"
     ]
    }
   ],
   "source": [
    "prepared_sentences = list()\n",
    "\n",
    "for sentence in sentences:\n",
    "    lengths = [len(w) for w in sentence.split(\" \")]\n",
    "    positions = []\n",
    "    \n",
    "    next_pos = 0\n",
    "    \n",
    "    for length in lengths:\n",
    "        next_pos = next_pos + length\n",
    "        positions.append(next_pos)\n",
    "    concatenated = sentence.replace(\" \", \"\")\n",
    "    chars = [c for c in concatenated]\n",
    "    labels = [0 if not i in positions else 1 for i,c in enumerate(concatenated)]\n",
    "    \n",
    "    prepared_sentences.append(list(zip(chars, labels)))\n",
    "    \n",
    "print([d for d in prepared_sentences[200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use n-grams\n",
    "# N-grams is based on the principle that you can predict the probability of occurence of the next word\n",
    "# based on the last N-1 words\n",
    "\n",
    "# creating features now, for CRF's\n",
    "# these features would probably be dependent on each other\n",
    "\n",
    "def create_char_features(sentence,i):\n",
    "    features = ['bias','char='+sentence[i][0]] \n",
    "    if i >= 1:\n",
    "        features.extend([\n",
    "            'char-1=' + sentence[i-1][0],\n",
    "            'char-1:0=' + sentence[i-1][0] + sentence[i][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"BOS\")\n",
    "        \n",
    "    if i >= 2:\n",
    "        features.extend([\n",
    "            'char-2=' + sentence[i-2][0],\n",
    "            'char-2:0=' + sentence[i-2][0] + sentence[i-1][0] + sentence[i][0],\n",
    "            'char-2:-1=' + sentence[i-2][0] + sentence[i-1][0],\n",
    "        ])\n",
    "        \n",
    "    if i >= 3:\n",
    "        features.extend([\n",
    "            'char-3:0=' + sentence[i-3][0] + sentence[i-2][0] + sentence[i-1][0] + sentence[i][0],\n",
    "            'char-3:-1=' + sentence[i-3][0] + sentence[i-2][0] + sentence[i-1][0],\n",
    "        ])\n",
    "    return features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing data\n",
    "# won't be using cross validation here\n",
    "\n",
    "def create_sentence_features(prepared_sentence):\n",
    "    return [create_char_features(prepared_sentence, i) for i in range(len(prepared_sentence))]\n",
    "\n",
    "def create_sentence_labels(prepared_sentence):\n",
    "    return [str(part[1]) for part in prepared_sentence]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "X = [create_sentence_features(ps) for ps in prepared_sentences[:-10000]]\n",
    "y = [create_sentence_labels(ps)   for ps in prepared_sentences[:-10000]]\n",
    "\n",
    "X_test = [create_sentence_features(ps) for ps in prepared_sentences[-10000:]]\n",
    "y_test = [create_sentence_labels(ps)   for ps in prepared_sentences[-10000:]]\n",
    "\n",
    "for xseq, yseq in zip(X,y):\n",
    "    trainer.append(xseq, yseq)\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1' : 1.0,\n",
    "    'c2' : 1e-3,\n",
    "    'max-iterations' : 60,\n",
    "    'feature.possible_transitions' : True\n",
    "})\n",
    "\n",
    "trainer.train('latin-text-segmentation.crfsuite')\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('latin-text-segmentation.crfsuite')\n",
    "\n",
    "def segment_sentence(sentence):\n",
    "    sent = sentence.replace(\" \", \"\")\n",
    "    prediction = tagger.tag(create_sentence_features(sent))\n",
    "    complete = \"\"\n",
    "    for i,p in enumerate(prediction):\n",
    "        if p=='1':\n",
    "            complete += \" \" + sent[i]\n",
    "        else:\n",
    "            complete += sent[i]\n",
    "    return complete\n",
    "\n",
    "print(segment_sentence(\"dominusadtemplumproperat\")) # dominus ad templum properat\n",
    "print(segment_sentence(\"portapatet\")) # porta patet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
